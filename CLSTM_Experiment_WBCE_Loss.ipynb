{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for available GPU.\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader functions\n",
    "# Inspiration: https://towardsdatascience.com/writing-custom-keras-generators-fe815d992c5a\n",
    "\n",
    "def get_input(path):\n",
    "    # Load array.\n",
    "    t_input = np.load(path)\n",
    "    # Pad to even number of pixels\n",
    "    t_input = np.pad(t_input, [(0,0),(0,1),(0,1)])\n",
    "    # Resize to include a channel dimension.\n",
    "    t_input = tf.expand_dims(t_input, axis = -1)\n",
    "    return t_input\n",
    "\n",
    "def get_output(path):\n",
    "    # Load array.\n",
    "    t_output = np.load(path)\n",
    "    # Pad to even number of pixels\n",
    "    t_output = np.pad(t_output, [(0,0),(0,1),(0,1)])\n",
    "    # Resize to include a channel dimension.\n",
    "    t_output = tf.expand_dims(t_output, axis = -1)\n",
    "    return t_output\n",
    "\n",
    "def data_generator(samples, batch_size = 64):\n",
    "    \n",
    "    while True:\n",
    "        # Select files (paths/indices) for the batch\n",
    "        batch_samples  = np.random.choice(a = samples.index, \n",
    "                                      size = batch_size)\n",
    "        batch_input  = []\n",
    "        batch_output = [] \n",
    "\n",
    "        # Read in each input, perform preprocessing and get labels\n",
    "        for sample in batch_samples:\n",
    "          input = get_input(samples.loc[sample].features)\n",
    "          output = get_output(samples.loc[sample].labels)\n",
    "\n",
    "          batch_input += [input]\n",
    "          batch_output += [output]\n",
    "        # Return a tuple of (input, output) to feed the network\n",
    "        batch_x = np.array(batch_input)\n",
    "        batch_y = np.array(batch_output)\n",
    "        \n",
    "        yield(batch_x, batch_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Lon</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Date</th>\n",
       "      <th>features</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>356962</td>\n",
       "      <td>-53.345535</td>\n",
       "      <td>-6.535028</td>\n",
       "      <td>2017-07-30</td>\n",
       "      <td>Sample_CLSTM_Dataset/train/features/356962.npy</td>\n",
       "      <td>Sample_CLSTM_Dataset/train/labels/356962.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>546517</td>\n",
       "      <td>-47.329685</td>\n",
       "      <td>-8.260676</td>\n",
       "      <td>2017-08-20</td>\n",
       "      <td>Sample_CLSTM_Dataset/train/features/546517.npy</td>\n",
       "      <td>Sample_CLSTM_Dataset/train/labels/546517.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>799359</td>\n",
       "      <td>-50.967861</td>\n",
       "      <td>-8.255809</td>\n",
       "      <td>2017-09-04</td>\n",
       "      <td>Sample_CLSTM_Dataset/train/features/799359.npy</td>\n",
       "      <td>Sample_CLSTM_Dataset/train/labels/799359.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>714590</td>\n",
       "      <td>-66.144379</td>\n",
       "      <td>-12.624272</td>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>Sample_CLSTM_Dataset/train/features/714590.npy</td>\n",
       "      <td>Sample_CLSTM_Dataset/train/labels/714590.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>240012</td>\n",
       "      <td>-48.350338</td>\n",
       "      <td>-11.838207</td>\n",
       "      <td>2017-07-14</td>\n",
       "      <td>Sample_CLSTM_Dataset/train/features/240012.npy</td>\n",
       "      <td>Sample_CLSTM_Dataset/train/labels/240012.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6491</th>\n",
       "      <td>173827</td>\n",
       "      <td>-54.326607</td>\n",
       "      <td>-11.983384</td>\n",
       "      <td>2017-06-20</td>\n",
       "      <td>Sample_CLSTM_Dataset/train/features/173827.npy</td>\n",
       "      <td>Sample_CLSTM_Dataset/train/labels/173827.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>1377764</td>\n",
       "      <td>-47.425915</td>\n",
       "      <td>-7.486426</td>\n",
       "      <td>2017-09-26</td>\n",
       "      <td>Sample_CLSTM_Dataset/train/features/1377764.npy</td>\n",
       "      <td>Sample_CLSTM_Dataset/train/labels/1377764.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>1444041</td>\n",
       "      <td>-65.172768</td>\n",
       "      <td>-10.714162</td>\n",
       "      <td>2017-10-02</td>\n",
       "      <td>Sample_CLSTM_Dataset/train/features/1444041.npy</td>\n",
       "      <td>Sample_CLSTM_Dataset/train/labels/1444041.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>644793</td>\n",
       "      <td>-53.701656</td>\n",
       "      <td>-15.977278</td>\n",
       "      <td>2017-08-27</td>\n",
       "      <td>Sample_CLSTM_Dataset/train/features/644793.npy</td>\n",
       "      <td>Sample_CLSTM_Dataset/train/labels/644793.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>1780752</td>\n",
       "      <td>-51.322929</td>\n",
       "      <td>-3.104345</td>\n",
       "      <td>2017-10-25</td>\n",
       "      <td>Sample_CLSTM_Dataset/train/features/1780752.npy</td>\n",
       "      <td>Sample_CLSTM_Dataset/train/labels/1780752.npy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6496 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0        Lon        Lat        Date  \\\n",
       "0         356962 -53.345535  -6.535028  2017-07-30   \n",
       "1         546517 -47.329685  -8.260676  2017-08-20   \n",
       "2         799359 -50.967861  -8.255809  2017-09-04   \n",
       "3         714590 -66.144379 -12.624272  2017-08-31   \n",
       "4         240012 -48.350338 -11.838207  2017-07-14   \n",
       "...          ...        ...        ...         ...   \n",
       "6491      173827 -54.326607 -11.983384  2017-06-20   \n",
       "6492     1377764 -47.425915  -7.486426  2017-09-26   \n",
       "6493     1444041 -65.172768 -10.714162  2017-10-02   \n",
       "6494      644793 -53.701656 -15.977278  2017-08-27   \n",
       "6495     1780752 -51.322929  -3.104345  2017-10-25   \n",
       "\n",
       "                                             features  \\\n",
       "0      Sample_CLSTM_Dataset/train/features/356962.npy   \n",
       "1      Sample_CLSTM_Dataset/train/features/546517.npy   \n",
       "2      Sample_CLSTM_Dataset/train/features/799359.npy   \n",
       "3      Sample_CLSTM_Dataset/train/features/714590.npy   \n",
       "4      Sample_CLSTM_Dataset/train/features/240012.npy   \n",
       "...                                               ...   \n",
       "6491   Sample_CLSTM_Dataset/train/features/173827.npy   \n",
       "6492  Sample_CLSTM_Dataset/train/features/1377764.npy   \n",
       "6493  Sample_CLSTM_Dataset/train/features/1444041.npy   \n",
       "6494   Sample_CLSTM_Dataset/train/features/644793.npy   \n",
       "6495  Sample_CLSTM_Dataset/train/features/1780752.npy   \n",
       "\n",
       "                                             labels  \n",
       "0      Sample_CLSTM_Dataset/train/labels/356962.npy  \n",
       "1      Sample_CLSTM_Dataset/train/labels/546517.npy  \n",
       "2      Sample_CLSTM_Dataset/train/labels/799359.npy  \n",
       "3      Sample_CLSTM_Dataset/train/labels/714590.npy  \n",
       "4      Sample_CLSTM_Dataset/train/labels/240012.npy  \n",
       "...                                             ...  \n",
       "6491   Sample_CLSTM_Dataset/train/labels/173827.npy  \n",
       "6492  Sample_CLSTM_Dataset/train/labels/1377764.npy  \n",
       "6493  Sample_CLSTM_Dataset/train/labels/1444041.npy  \n",
       "6494   Sample_CLSTM_Dataset/train/labels/644793.npy  \n",
       "6495  Sample_CLSTM_Dataset/train/labels/1780752.npy  \n",
       "\n",
       "[6496 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = pd.read_csv('Sample_CLSTM_Dataset/train/meta.csv')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 10, 32, 32, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get data from one chip\n",
    "sample_input = np.load('Sample_CLSTM_Dataset/train/features/3243.npy')\n",
    "\n",
    "# Pad to even number of pixels\n",
    "a = np.pad(sample_input, [(0,0),(0,1),(0,1)])\n",
    "# Resize to include a channel dimension.\n",
    "a = tf.expand_dims(a, axis = -1)\n",
    "# Resize to include a batch dimension.\n",
    "a = tf.expand_dims(a, axis = 0)\n",
    "# Display shape for verification.\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model assumbly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by: https://keras.io/examples/vision/conv_lstm/?fbclid=IwAR1QSJmF0bcz5pklHDHwl4mUi8inzHN4m8Zk6OpvVJizqDDv2-MKMq3LlJ8\n",
    "\n",
    "inputs = layers.Input(shape=((10,32,32,1)))\n",
    "x = layers.ConvLSTM2D(\n",
    "    filters=64,\n",
    "    kernel_size=(5, 5),\n",
    "    padding='same',\n",
    "    return_sequences=True,\n",
    "    activation='relu',\n",
    ")(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ConvLSTM2D(\n",
    "    filters=64,\n",
    "    kernel_size=(3, 3),\n",
    "    padding='same',\n",
    "    return_sequences=True,\n",
    "    activation='relu',\n",
    ")(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ConvLSTM2D(\n",
    "    filters=64,\n",
    "    kernel_size=(1, 1),\n",
    "    padding='same',\n",
    "    return_sequences=True,\n",
    "    activation='relu',\n",
    ")(x)\n",
    "outputs = layers.Conv3D(\n",
    "    filters=1, kernel_size=(3, 3, 3), activation='sigmoid', padding='same'\n",
    ")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_model = tf.keras.Model(inputs, outputs, name=\"Conv_LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 10, 32, 32, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify output shape.\n",
    "forecast_model.predict(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Conv_LSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 10, 32, 32, 1)]   0         \n",
      "                                                                 \n",
      " conv_lstm2d (ConvLSTM2D)    (None, 10, 32, 32, 64)    416256    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 10, 32, 32, 64)   256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv_lstm2d_1 (ConvLSTM2D)  (None, 10, 32, 32, 64)    295168    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 10, 32, 32, 64)   256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv_lstm2d_2 (ConvLSTM2D)  (None, 10, 32, 32, 64)    33024     \n",
      "                                                                 \n",
      " conv3d (Conv3D)             (None, 10, 32, 32, 1)     1729      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 746,689\n",
      "Trainable params: 746,433\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display model details.\n",
    "forecast_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model.\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "forecast_model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders.\n",
    "batch_size = 64\n",
    "meta_t = pd.read_csv('Sample_CLSTM_Dataset/train/meta.csv')\n",
    "meta_v = pd.read_csv('Sample_CLSTM_Dataset/val/meta.csv')\n",
    "t_gen = data_generator(meta_t, batch_size = batch_size)\n",
    "v_gen = data_generator(meta_v, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying new approach to build dataset.\n",
    "x_train, y_train, x_val, y_val = [],[],[],[]\n",
    "\n",
    "# Iterate over dataset.\n",
    "for x in range(0,len(meta_t)):\n",
    "    x_train.append(np.load(meta_t.iloc[0].features))\n",
    "    y_train.append(np.load(meta_t.iloc[0].labels))\n",
    "for x in range(0,len(meta_v)):\n",
    "    x_val.append(np.load(meta_v.iloc[0].features))\n",
    "    y_val.append(np.load(meta_v.iloc[0].labels))\n",
    "    \n",
    "# Stack layers.\n",
    "x_train = np.stack(x_train)\n",
    "y_train = np.stack(y_train)\n",
    "x_val = np.stack(x_val)\n",
    "y_val = np.stack(y_val)\n",
    "\n",
    "# Convert values > 1 to 1.\n",
    "x_train = np.minimum(x_train,1)\n",
    "y_train = np.minimum(y_train,1)\n",
    "x_val = np.minimum(x_val,1)\n",
    "y_val = np.minimum(y_val,1)\n",
    "\n",
    "# Pad to even shape.\n",
    "x_train = np.pad(x_train, [(0,0),(0,0),(0,1),(0,1)])\n",
    "y_train = np.pad(y_train, [(0,0),(0,0),(0,1),(0,1)])\n",
    "x_val = np.pad(x_val, [(0,0),(0,0),(0,1),(0,1)])\n",
    "y_val = np.pad(y_val, [(0,0),(0,0),(0,1),(0,1)])\n",
    "\n",
    "# Add color channel dim.\n",
    "x_train = tf.expand_dims(x_train, axis = -1)\n",
    "y_train = tf.expand_dims(y_train, axis = -1)\n",
    "x_val = tf.expand_dims(x_val, axis = -1)\n",
    "y_val = tf.expand_dims(y_val, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution for problem with class_weights not working with 3D outputs in tensorflow.\n",
    "# From: https://github.com/keras-team/keras/issues/3653\n",
    "def generate_sample_weights(training_data, class_weights): \n",
    "    #replaces values for up to 3 classes with the values from class_weights#\n",
    "    sample_weights = [np.where(y==0,class_weights[0],\n",
    "                        np.where(y==1,class_weights[1],\n",
    "                        y)) for y in training_data]\n",
    "    return np.asarray(sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.5010275 , 243.80952381])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get class weights for weighted binary cross entropy loss.\n",
    "weights = class_weight.compute_class_weight('balanced',\n",
    "                                            [0,1],\n",
    "                                            y_train.numpy().flatten())\n",
    "\n",
    "#weights = {i : weights[i] for i in range(0,2)}\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculated_sample_weights = generate_sample_weights(y_train, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "203/203 [==============================] - 64s 302ms/step - loss: 0.6559 - accuracy: 0.6826 - val_loss: 0.6822 - val_accuracy: 0.9999 - lr: 1.0000e-05\n",
      "Epoch 2/50\n",
      "203/203 [==============================] - 61s 300ms/step - loss: 0.4184 - accuracy: 0.8725 - val_loss: 0.4116 - val_accuracy: 0.8908 - lr: 1.0000e-05\n",
      "Epoch 3/50\n",
      "203/203 [==============================] - 61s 301ms/step - loss: 0.3251 - accuracy: 0.8851 - val_loss: 0.2050 - val_accuracy: 0.8999 - lr: 1.0000e-05\n",
      "Epoch 4/50\n",
      "203/203 [==============================] - 61s 300ms/step - loss: 0.2690 - accuracy: 0.8564 - val_loss: 0.2041 - val_accuracy: 0.9022 - lr: 1.0000e-05\n",
      "Epoch 5/50\n",
      "203/203 [==============================] - 61s 301ms/step - loss: 0.2403 - accuracy: 0.8473 - val_loss: 0.2417 - val_accuracy: 0.9009 - lr: 1.0000e-05\n",
      "Epoch 6/50\n",
      "203/203 [==============================] - 61s 301ms/step - loss: 0.2239 - accuracy: 0.8269 - val_loss: 0.2386 - val_accuracy: 0.8057 - lr: 1.0000e-05\n",
      "Epoch 7/50\n",
      "203/203 [==============================] - 61s 301ms/step - loss: 0.2132 - accuracy: 0.8050 - val_loss: 0.2096 - val_accuracy: 0.8624 - lr: 1.0000e-05\n",
      "Epoch 8/50\n",
      "203/203 [==============================] - 61s 301ms/step - loss: 0.2049 - accuracy: 0.8111 - val_loss: 0.1945 - val_accuracy: 0.9212 - lr: 1.0000e-05\n",
      "Epoch 9/50\n",
      "203/203 [==============================] - 61s 301ms/step - loss: 0.1983 - accuracy: 0.8158 - val_loss: 0.2089 - val_accuracy: 0.8220 - lr: 1.0000e-05\n",
      "Epoch 10/50\n",
      "203/203 [==============================] - 61s 301ms/step - loss: 0.1936 - accuracy: 0.8212 - val_loss: 0.1980 - val_accuracy: 0.8306 - lr: 1.0000e-05\n",
      "Epoch 11/50\n",
      "203/203 [==============================] - 61s 300ms/step - loss: 0.1905 - accuracy: 0.8235 - val_loss: 0.2151 - val_accuracy: 0.8246 - lr: 1.0000e-05\n",
      "Epoch 12/50\n",
      "203/203 [==============================] - 61s 301ms/step - loss: 0.1875 - accuracy: 0.8253 - val_loss: 0.2006 - val_accuracy: 0.8323 - lr: 1.0000e-05\n",
      "Epoch 13/50\n",
      "203/203 [==============================] - 61s 301ms/step - loss: 0.1853 - accuracy: 0.8271 - val_loss: 0.1864 - val_accuracy: 0.8379 - lr: 1.0000e-05\n",
      "Epoch 14/50\n",
      "203/203 [==============================] - 61s 301ms/step - loss: 0.1836 - accuracy: 0.8307 - val_loss: 0.2104 - val_accuracy: 0.8341 - lr: 1.0000e-05\n",
      "Epoch 15/50\n",
      "203/203 [==============================] - 61s 301ms/step - loss: 0.1821 - accuracy: 0.8325 - val_loss: 0.2201 - val_accuracy: 0.8322 - lr: 1.0000e-05\n",
      "Epoch 16/50\n",
      "203/203 [==============================] - 61s 301ms/step - loss: 0.1809 - accuracy: 0.8339 - val_loss: 0.2284 - val_accuracy: 0.8307 - lr: 1.0000e-05\n",
      "Epoch 17/50\n",
      "203/203 [==============================] - 61s 299ms/step - loss: 0.1797 - accuracy: 0.8356 - val_loss: 0.1793 - val_accuracy: 0.8472 - lr: 1.0000e-05\n",
      "Epoch 18/50\n",
      "203/203 [==============================] - 61s 299ms/step - loss: 0.1787 - accuracy: 0.8373 - val_loss: 0.2346 - val_accuracy: 0.8336 - lr: 1.0000e-05\n",
      "Epoch 19/50\n",
      "203/203 [==============================] - 61s 299ms/step - loss: 0.1774 - accuracy: 0.8391 - val_loss: 0.1875 - val_accuracy: 0.8477 - lr: 1.0000e-05\n",
      "Epoch 20/50\n",
      "203/203 [==============================] - 61s 299ms/step - loss: 0.1768 - accuracy: 0.8424 - val_loss: 0.1879 - val_accuracy: 0.8469 - lr: 1.0000e-05\n",
      "Epoch 21/50\n",
      "203/203 [==============================] - 61s 299ms/step - loss: 0.1764 - accuracy: 0.8434 - val_loss: 0.2242 - val_accuracy: 0.8430 - lr: 1.0000e-05\n",
      "Epoch 22/50\n",
      "203/203 [==============================] - 61s 299ms/step - loss: 0.1764 - accuracy: 0.8441 - val_loss: 0.1684 - val_accuracy: 0.8764 - lr: 1.0000e-05\n",
      "Epoch 23/50\n",
      "203/203 [==============================] - 61s 300ms/step - loss: 0.1738 - accuracy: 0.8483 - val_loss: 0.1927 - val_accuracy: 0.8488 - lr: 1.0000e-05\n",
      "Epoch 24/50\n",
      "203/203 [==============================] - 61s 299ms/step - loss: 0.1730 - accuracy: 0.8488 - val_loss: 0.2115 - val_accuracy: 0.8472 - lr: 1.0000e-05\n",
      "Epoch 25/50\n",
      "203/203 [==============================] - 61s 302ms/step - loss: 0.1729 - accuracy: 0.8490 - val_loss: 0.2440 - val_accuracy: 0.8457 - lr: 1.0000e-05\n",
      "Epoch 26/50\n",
      "203/203 [==============================] - 61s 301ms/step - loss: 0.1719 - accuracy: 0.8490 - val_loss: 0.3148 - val_accuracy: 0.8406 - lr: 1.0000e-05\n",
      "Epoch 27/50\n",
      "203/203 [==============================] - 61s 301ms/step - loss: 0.1710 - accuracy: 0.8494 - val_loss: 0.2262 - val_accuracy: 0.8467 - lr: 1.0000e-05\n",
      "Epoch 28/50\n",
      "203/203 [==============================] - 61s 301ms/step - loss: 0.1696 - accuracy: 0.8493 - val_loss: 0.2010 - val_accuracy: 0.8502 - lr: 1.0000e-06\n",
      "Epoch 29/50\n",
      "203/203 [==============================] - 61s 301ms/step - loss: 0.1693 - accuracy: 0.8496 - val_loss: 0.2080 - val_accuracy: 0.8501 - lr: 1.0000e-06\n",
      "Epoch 30/50\n",
      "203/203 [==============================] - 61s 301ms/step - loss: 0.1691 - accuracy: 0.8502 - val_loss: 0.2074 - val_accuracy: 0.8502 - lr: 1.0000e-06\n",
      "Epoch 31/50\n",
      "203/203 [==============================] - 61s 301ms/step - loss: 0.1690 - accuracy: 0.8505 - val_loss: 0.2095 - val_accuracy: 0.8501 - lr: 1.0000e-06\n",
      "Epoch 32/50\n",
      "203/203 [==============================] - 61s 301ms/step - loss: 0.1689 - accuracy: 0.8510 - val_loss: 0.2113 - val_accuracy: 0.8501 - lr: 1.0000e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2aa4195a1c0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Params\n",
    "batch_size = 32\n",
    "\n",
    "# Adding callbacks.\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5)\n",
    "\n",
    "# Train model.\n",
    "forecast_model.fit(#t_gen, \n",
    "                   x_train,\n",
    "                   y_train,\n",
    "                   epochs = 50, \n",
    "                   verbose = 1, \n",
    "                   batch_size = batch_size,\n",
    "                   #validation_data = v_gen, \n",
    "                   validation_data = (x_val, y_val),\n",
    "                   #steps_per_epoch = len(meta_t) // batch_size,\n",
    "                   #validation_steps = len(meta_v) // batch_size,\n",
    "                   callbacks = [early_stopping, reduce_lr],\n",
    "                   #class_weight = weights\n",
    "                   sample_weight = calculated_sample_weights\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/008\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/008\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save trained model.\n",
    "forecast_model.save('Models/008')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_model = tf.keras.models.load_model('Models/008')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine how model is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n"
     ]
    }
   ],
   "source": [
    "# Select a random example from the validation dataset.\n",
    "example = np.random.choice(range(len(x_val)), size=1)[0]\n",
    "\n",
    "# Pick the first/last ten frames from the example.\n",
    "feature_frames = x_val[example, ...]\n",
    "label_frames = y_val[example, ...]\n",
    "pred_frames = []\n",
    "\n",
    "# Predict a new set of 10 frames.\n",
    "for _ in range(10):\n",
    "    # Extract the model's prediction and post-process it.\n",
    "    new_prediction = forecast_model.predict(np.expand_dims(feature_frames, axis=0))\n",
    "    new_prediction = np.squeeze(new_prediction, axis=0)\n",
    "    predicted_frame = np.expand_dims(new_prediction[-1, ...], axis=0)\n",
    "    pred_frames.append(predicted_frame)\n",
    "    \n",
    "\n",
    "# Extend the set of prediction frames.\n",
    "pred_frames = np.concatenate(pred_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape verification.\n",
    "feature_frames.shape == label_frames.shape == pred_frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x576 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Construct a figure for the original and new frames.\n",
    "fig, axes = plt.subplots(3, 10, figsize=(30, 8))\n",
    "\n",
    "# Plot the feature frames.\n",
    "for idx, ax in enumerate(axes[0]):\n",
    "    ax.imshow(np.squeeze(feature_frames[idx]), cmap=\"gray\")\n",
    "    ax.set_title(f\"F Frame {idx + 1}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Plot the label frames.\n",
    "for idx, ax in enumerate(axes[1]):\n",
    "    ax.imshow(np.squeeze(label_frames[idx]), cmap=\"gray\")\n",
    "    ax.set_title(f\"L Frame {idx + 11}\")\n",
    "    ax.axis(\"off\")\n",
    "    \n",
    "# Plot the predicted frames.\n",
    "for idx, ax in enumerate(axes[2]):\n",
    "    ax.imshow(np.squeeze(pred_frames[idx]), cmap=\"gray\")\n",
    "    ax.set_title(f\"P Frame {idx + 11}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "# Display the figure.\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
