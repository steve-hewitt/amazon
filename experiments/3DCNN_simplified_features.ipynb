{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.initializers import Constant\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for available GPU.\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader functions\n",
    "# Inspiration: https://towardsdatascience.com/writing-custom-keras-generators-fe815d992c5a\n",
    "\n",
    "def get_input(path):\n",
    "    # Load array.\n",
    "    t_input = np.load(path)\n",
    "    # Move channel axis to the end.\n",
    "    t_input = np.moveaxis(t_input, 0, -1)\n",
    "    # Pad to even number of pixels\n",
    "    t_input = np.pad(t_input, [(0,0),(0,1),(0,1),(0,0)])\n",
    "    return t_input\n",
    "\n",
    "def get_output(path):\n",
    "    # Load array.\n",
    "    t_output = np.load(path)\n",
    "    # Pad to even number of pixels\n",
    "    #t_output = np.pad(t_output, [(0,0),(0,1),(0,1)])\n",
    "    # Resize to include a channel dimension.\n",
    "    #t_output = tf.expand_dims(t_output, axis = -1)\n",
    "    t_output = t_output.sum()\n",
    "    return t_output\n",
    "\n",
    "def data_generator(samples, batch_size = 64):\n",
    "    \n",
    "    while True:\n",
    "        # Select files (paths/indices) for the batch\n",
    "        batch_samples  = np.random.choice(a = samples.index, \n",
    "                                      size = batch_size)\n",
    "        batch_input  = []\n",
    "        batch_output = [] \n",
    "\n",
    "        # Read in each input, perform preprocessing and get labels\n",
    "        for sample in batch_samples:\n",
    "          input = get_input(samples.loc[sample].features)\n",
    "          output = get_output(samples.loc[sample].labels)\n",
    "\n",
    "          batch_input += [input]\n",
    "          batch_output += [output]\n",
    "        # Return a tuple of (input, output) to feed the network\n",
    "        batch_x = np.array(batch_input)\n",
    "        batch_y = np.array(batch_output)\n",
    "        \n",
    "        yield(batch_x, batch_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Lon</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Date</th>\n",
       "      <th>features</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>356962</td>\n",
       "      <td>-53.345535</td>\n",
       "      <td>-6.535028</td>\n",
       "      <td>2017-07-30</td>\n",
       "      <td>Sample_Dataset_2/train/features/356962.npy</td>\n",
       "      <td>Sample_Dataset_2/train/labels/356962.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>546517</td>\n",
       "      <td>-47.329685</td>\n",
       "      <td>-8.260676</td>\n",
       "      <td>2017-08-20</td>\n",
       "      <td>Sample_Dataset_2/train/features/546517.npy</td>\n",
       "      <td>Sample_Dataset_2/train/labels/546517.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>799359</td>\n",
       "      <td>-50.967861</td>\n",
       "      <td>-8.255809</td>\n",
       "      <td>2017-09-04</td>\n",
       "      <td>Sample_Dataset_2/train/features/799359.npy</td>\n",
       "      <td>Sample_Dataset_2/train/labels/799359.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>714590</td>\n",
       "      <td>-66.144379</td>\n",
       "      <td>-12.624272</td>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>Sample_Dataset_2/train/features/714590.npy</td>\n",
       "      <td>Sample_Dataset_2/train/labels/714590.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>240012</td>\n",
       "      <td>-48.350338</td>\n",
       "      <td>-11.838207</td>\n",
       "      <td>2017-07-14</td>\n",
       "      <td>Sample_Dataset_2/train/features/240012.npy</td>\n",
       "      <td>Sample_Dataset_2/train/labels/240012.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6491</th>\n",
       "      <td>173827</td>\n",
       "      <td>-54.326607</td>\n",
       "      <td>-11.983384</td>\n",
       "      <td>2017-06-20</td>\n",
       "      <td>Sample_Dataset_2/train/features/173827.npy</td>\n",
       "      <td>Sample_Dataset_2/train/labels/173827.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>1377764</td>\n",
       "      <td>-47.425915</td>\n",
       "      <td>-7.486426</td>\n",
       "      <td>2017-09-26</td>\n",
       "      <td>Sample_Dataset_2/train/features/1377764.npy</td>\n",
       "      <td>Sample_Dataset_2/train/labels/1377764.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>1444041</td>\n",
       "      <td>-65.172768</td>\n",
       "      <td>-10.714162</td>\n",
       "      <td>2017-10-02</td>\n",
       "      <td>Sample_Dataset_2/train/features/1444041.npy</td>\n",
       "      <td>Sample_Dataset_2/train/labels/1444041.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6494</th>\n",
       "      <td>644793</td>\n",
       "      <td>-53.701656</td>\n",
       "      <td>-15.977278</td>\n",
       "      <td>2017-08-27</td>\n",
       "      <td>Sample_Dataset_2/train/features/644793.npy</td>\n",
       "      <td>Sample_Dataset_2/train/labels/644793.npy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6495</th>\n",
       "      <td>1780752</td>\n",
       "      <td>-51.322929</td>\n",
       "      <td>-3.104345</td>\n",
       "      <td>2017-10-25</td>\n",
       "      <td>Sample_Dataset_2/train/features/1780752.npy</td>\n",
       "      <td>Sample_Dataset_2/train/labels/1780752.npy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6496 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0        Lon        Lat        Date  \\\n",
       "0         356962 -53.345535  -6.535028  2017-07-30   \n",
       "1         546517 -47.329685  -8.260676  2017-08-20   \n",
       "2         799359 -50.967861  -8.255809  2017-09-04   \n",
       "3         714590 -66.144379 -12.624272  2017-08-31   \n",
       "4         240012 -48.350338 -11.838207  2017-07-14   \n",
       "...          ...        ...        ...         ...   \n",
       "6491      173827 -54.326607 -11.983384  2017-06-20   \n",
       "6492     1377764 -47.425915  -7.486426  2017-09-26   \n",
       "6493     1444041 -65.172768 -10.714162  2017-10-02   \n",
       "6494      644793 -53.701656 -15.977278  2017-08-27   \n",
       "6495     1780752 -51.322929  -3.104345  2017-10-25   \n",
       "\n",
       "                                         features  \\\n",
       "0      Sample_Dataset_2/train/features/356962.npy   \n",
       "1      Sample_Dataset_2/train/features/546517.npy   \n",
       "2      Sample_Dataset_2/train/features/799359.npy   \n",
       "3      Sample_Dataset_2/train/features/714590.npy   \n",
       "4      Sample_Dataset_2/train/features/240012.npy   \n",
       "...                                           ...   \n",
       "6491   Sample_Dataset_2/train/features/173827.npy   \n",
       "6492  Sample_Dataset_2/train/features/1377764.npy   \n",
       "6493  Sample_Dataset_2/train/features/1444041.npy   \n",
       "6494   Sample_Dataset_2/train/features/644793.npy   \n",
       "6495  Sample_Dataset_2/train/features/1780752.npy   \n",
       "\n",
       "                                         labels  \n",
       "0      Sample_Dataset_2/train/labels/356962.npy  \n",
       "1      Sample_Dataset_2/train/labels/546517.npy  \n",
       "2      Sample_Dataset_2/train/labels/799359.npy  \n",
       "3      Sample_Dataset_2/train/labels/714590.npy  \n",
       "4      Sample_Dataset_2/train/labels/240012.npy  \n",
       "...                                         ...  \n",
       "6491   Sample_Dataset_2/train/labels/173827.npy  \n",
       "6492  Sample_Dataset_2/train/labels/1377764.npy  \n",
       "6493  Sample_Dataset_2/train/labels/1444041.npy  \n",
       "6494   Sample_Dataset_2/train/labels/644793.npy  \n",
       "6495  Sample_Dataset_2/train/labels/1780752.npy  \n",
       "\n",
       "[6496 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = pd.read_csv('Sample_Dataset_2/train/meta.csv')\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 4, 32, 32, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get data from one chip\n",
    "sample_input = np.load('Sample_Dataset_2/train/features/3243.npy')\n",
    "\n",
    "# Move channel axis to the end.\n",
    "sample_input = np.moveaxis(sample_input, 0, -1)\n",
    "# Pad to even number of pixels\n",
    "a = np.pad(sample_input, [(0,0),(0,1),(0,1),(0,0)])\n",
    "# Resize to include a batch dimension.\n",
    "a = tf.expand_dims(a, axis = 0)\n",
    "# Display shape for verification.\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment with basic model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 4, 32, 32, 64])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = tf.keras.layers.Conv3D(64, (3,3,3), padding = 'same', activation='relu', bias_initializer=Constant(0.01), \n",
    "                           input_shape=(a))(a)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 4, 32, 32, 64])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = tf.keras.layers.Conv3D(64, (3,3,3), padding = 'same', activation='relu', bias_initializer=Constant(0.01) \n",
    "                           )(b)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 2, 16, 16, 64])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = layers.MaxPooling3D((2,2,2))(c)\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 2, 16, 16, 128])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = tf.keras.layers.Conv3D(128, (3,3,3), padding = 'same', activation='relu')(d)\n",
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 2, 16, 16, 128])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = tf.keras.layers.Conv3D(128, (3,3,3), padding = 'same', activation='relu')(e)\n",
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 2, 8, 8, 128])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = layers.MaxPooling3D((1,2,2))(f)\n",
    "g.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 2, 8, 8, 256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = tf.keras.layers.Conv3D(256, (3,3,3), padding = 'same', activation='relu')(g)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 2, 8, 8, 256])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = tf.keras.layers.Conv3D(256, (3,3,3), padding = 'same', activation='relu')(h)\n",
    "i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 2, 4, 4, 256])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j = layers.MaxPooling3D((1,2,2))(i)\n",
    "j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 2, 4, 4, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = tf.keras.layers.Conv3D(512, (3,3,3), padding = 'same', activation='relu')(j)\n",
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 2, 4, 4, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = tf.keras.layers.Conv3D(512, (3,3,3), padding = 'same', activation='relu')(k)\n",
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 1, 1, 1, 512])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = layers.MaxPooling3D((2,4,4))(l)\n",
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 512])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = layers.Flatten()(m)\n",
    "n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 64])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = layers.Dense(64, activation='relu')(n)\n",
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 32])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = layers.Dense(32, activation='relu')(o)\n",
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 16])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = layers.Dense(16, activation='relu')(p)\n",
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = layers.Dense(1, activation='linear')(q)\n",
    "r.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model assumbly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=((4,32,32,4)))\n",
    "b = tf.keras.layers.Conv3D(32, (3,3,3), padding = 'same', activation='gelu', bias_initializer=Constant(0.01))(inputs)\n",
    "c = tf.keras.layers.Conv3D(32, (3,3,3), padding = 'same', activation='gelu', bias_initializer=Constant(0.01))(b)\n",
    "d = layers.MaxPooling3D((2,2,2))(c)\n",
    "d = layers.BatchNormalization()(d)\n",
    "d = layers.Dropout(0.3)(d)\n",
    "e = tf.keras.layers.Conv3D(64, (3,3,3), padding = 'same', activation='gelu')(d)\n",
    "f = tf.keras.layers.Conv3D(64, (3,3,3), padding = 'same', activation='gelu')(e)\n",
    "g = layers.MaxPooling3D((1,2,2))(f)\n",
    "g = layers.BatchNormalization()(g)\n",
    "g = layers.Dropout(0.3)(g)\n",
    "h = tf.keras.layers.Conv3D(128, (3,3,3), padding = 'same', activation='gelu')(g)\n",
    "i = tf.keras.layers.Conv3D(128, (3,3,3), padding = 'same', activation='gelu')(h)\n",
    "j = layers.MaxPooling3D((1,2,2))(i)\n",
    "j = layers.BatchNormalization()(j)\n",
    "j = layers.Dropout(0.3)(j)\n",
    "k = tf.keras.layers.Conv3D(256, (3,3,3), padding = 'same', activation='gelu')(j)\n",
    "l = tf.keras.layers.Conv3D(256, (3,3,3), padding = 'same', activation='gelu')(k)\n",
    "m = layers.MaxPooling3D((2,4,4))(l)\n",
    "m = layers.BatchNormalization()(m)\n",
    "m = layers.Dropout(0.3)(m)\n",
    "n = layers.Flatten()(m)\n",
    "o = layers.Dense(1024, activation='gelu')(n)\n",
    "o = layers.BatchNormalization()(o)\n",
    "o = layers.Dropout(0.3)(o)\n",
    "p = layers.Dense(1024, activation='gelu')(o)\n",
    "p = layers.BatchNormalization()(p)\n",
    "p = layers.Dropout(0.3)(p)\n",
    "q = layers.Dense(1024, activation='gelu')(p)\n",
    "q = layers.BatchNormalization()(q)\n",
    "q = layers.Dropout(0.3)(q)\n",
    "outputs = layers.Dense(1, activation='linear')(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_model = tf.keras.Model(inputs, outputs, name=\"3D_CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 194ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify output shape.\n",
    "forecast_model.predict(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"3D_CNN\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 4, 32, 32, 4)]    0         \n",
      "                                                                 \n",
      " conv3d_8 (Conv3D)           (None, 4, 32, 32, 32)     3488      \n",
      "                                                                 \n",
      " conv3d_9 (Conv3D)           (None, 4, 32, 32, 32)     27680     \n",
      "                                                                 \n",
      " max_pooling3d_4 (MaxPooling  (None, 2, 16, 16, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 2, 16, 16, 32)    128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 2, 16, 16, 32)     0         \n",
      "                                                                 \n",
      " conv3d_10 (Conv3D)          (None, 2, 16, 16, 64)     55360     \n",
      "                                                                 \n",
      " conv3d_11 (Conv3D)          (None, 2, 16, 16, 64)     110656    \n",
      "                                                                 \n",
      " max_pooling3d_5 (MaxPooling  (None, 2, 8, 8, 64)      0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 2, 8, 8, 64)      256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 2, 8, 8, 64)       0         \n",
      "                                                                 \n",
      " conv3d_12 (Conv3D)          (None, 2, 8, 8, 128)      221312    \n",
      "                                                                 \n",
      " conv3d_13 (Conv3D)          (None, 2, 8, 8, 128)      442496    \n",
      "                                                                 \n",
      " max_pooling3d_6 (MaxPooling  (None, 2, 4, 4, 128)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 2, 4, 4, 128)     512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 2, 4, 4, 128)      0         \n",
      "                                                                 \n",
      " conv3d_14 (Conv3D)          (None, 2, 4, 4, 256)      884992    \n",
      "                                                                 \n",
      " conv3d_15 (Conv3D)          (None, 2, 4, 4, 256)      1769728   \n",
      "                                                                 \n",
      " max_pooling3d_7 (MaxPooling  (None, 1, 1, 1, 256)     0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 1, 1, 1, 256)     1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1, 1, 1, 256)      0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1024)              263168    \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 1024)             4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1024)              1049600   \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 1024)             4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1024)              1049600   \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 1024)             4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,893,313\n",
      "Trainable params: 5,886,209\n",
      "Non-trainable params: 7,104\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display model details.\n",
    "forecast_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model.\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "forecast_model.compile(loss = tf.keras.losses.MeanSquaredError(), \n",
    "                       optimizer=opt, \n",
    "                       metrics = [tf.keras.metrics.MeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders.\n",
    "batch_size = 64\n",
    "meta_t = pd.read_csv('Sample_Dataset_2/train/meta.csv')\n",
    "meta_v = pd.read_csv('Sample_Dataset_2/val/meta.csv')\n",
    "t_gen = data_generator(meta_t, batch_size = batch_size)\n",
    "v_gen = data_generator(meta_v, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "101/101 [==============================] - 45s 433ms/step - loss: 427.5701 - mean_squared_error: 427.5701 - val_loss: 268.1029 - val_mean_squared_error: 268.1029\n",
      "Epoch 2/50\n",
      "101/101 [==============================] - 20s 203ms/step - loss: 409.1885 - mean_squared_error: 409.1885 - val_loss: 300.0758 - val_mean_squared_error: 300.0758\n",
      "Epoch 3/50\n",
      "101/101 [==============================] - 12s 116ms/step - loss: 404.4841 - mean_squared_error: 404.4841 - val_loss: 357.5589 - val_mean_squared_error: 357.5589\n",
      "Epoch 4/50\n",
      "101/101 [==============================] - 9s 90ms/step - loss: 389.3833 - mean_squared_error: 389.3833 - val_loss: 338.6359 - val_mean_squared_error: 338.6359\n",
      "Epoch 5/50\n",
      "101/101 [==============================] - 8s 75ms/step - loss: 404.1831 - mean_squared_error: 404.1831 - val_loss: 309.6389 - val_mean_squared_error: 309.6389\n",
      "Epoch 6/50\n",
      "101/101 [==============================] - 7s 71ms/step - loss: 390.4630 - mean_squared_error: 390.4630 - val_loss: 512.6423 - val_mean_squared_error: 512.6423\n",
      "Epoch 7/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 424.0310 - mean_squared_error: 424.0310 - val_loss: 391.2163 - val_mean_squared_error: 391.2163\n",
      "Epoch 8/50\n",
      "101/101 [==============================] - 7s 68ms/step - loss: 361.3730 - mean_squared_error: 361.3730 - val_loss: 326.6401 - val_mean_squared_error: 326.6401\n",
      "Epoch 9/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 331.2539 - mean_squared_error: 331.2539 - val_loss: 273.5452 - val_mean_squared_error: 273.5452\n",
      "Epoch 10/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 360.1634 - mean_squared_error: 360.1634 - val_loss: 374.5589 - val_mean_squared_error: 374.5589\n",
      "Epoch 11/50\n",
      "101/101 [==============================] - 7s 68ms/step - loss: 400.5945 - mean_squared_error: 400.5945 - val_loss: 287.4334 - val_mean_squared_error: 287.4334\n",
      "Epoch 12/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 369.2768 - mean_squared_error: 369.2768 - val_loss: 311.4749 - val_mean_squared_error: 311.4749\n",
      "Epoch 13/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 367.7579 - mean_squared_error: 367.7579 - val_loss: 377.3233 - val_mean_squared_error: 377.3233\n",
      "Epoch 14/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 422.4991 - mean_squared_error: 422.4991 - val_loss: 264.8872 - val_mean_squared_error: 264.8872\n",
      "Epoch 15/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 378.2431 - mean_squared_error: 378.2431 - val_loss: 304.3057 - val_mean_squared_error: 304.3057\n",
      "Epoch 16/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 384.4366 - mean_squared_error: 384.4366 - val_loss: 360.5070 - val_mean_squared_error: 360.5070\n",
      "Epoch 17/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 399.4930 - mean_squared_error: 399.4930 - val_loss: 326.1350 - val_mean_squared_error: 326.1350\n",
      "Epoch 18/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 364.8016 - mean_squared_error: 364.8016 - val_loss: 222.4998 - val_mean_squared_error: 222.4998\n",
      "Epoch 19/50\n",
      "101/101 [==============================] - 7s 68ms/step - loss: 362.9522 - mean_squared_error: 362.9522 - val_loss: 252.0397 - val_mean_squared_error: 252.0397\n",
      "Epoch 20/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 373.0173 - mean_squared_error: 373.0173 - val_loss: 332.9297 - val_mean_squared_error: 332.9297\n",
      "Epoch 21/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 396.8977 - mean_squared_error: 396.8977 - val_loss: 257.8515 - val_mean_squared_error: 257.8515\n",
      "Epoch 22/50\n",
      "101/101 [==============================] - 7s 68ms/step - loss: 365.7943 - mean_squared_error: 365.7943 - val_loss: 275.0329 - val_mean_squared_error: 275.0329\n",
      "Epoch 23/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 371.6399 - mean_squared_error: 371.6399 - val_loss: 257.1787 - val_mean_squared_error: 257.1787\n",
      "Epoch 24/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 327.3515 - mean_squared_error: 327.3515 - val_loss: 312.4722 - val_mean_squared_error: 312.4722\n",
      "Epoch 25/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 335.9074 - mean_squared_error: 335.9074 - val_loss: 286.4495 - val_mean_squared_error: 286.4495\n",
      "Epoch 26/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 381.1767 - mean_squared_error: 381.1767 - val_loss: 259.2417 - val_mean_squared_error: 259.2417\n",
      "Epoch 27/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 339.1783 - mean_squared_error: 339.1783 - val_loss: 334.2116 - val_mean_squared_error: 334.2116\n",
      "Epoch 28/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 344.7712 - mean_squared_error: 344.7712 - val_loss: 309.3899 - val_mean_squared_error: 309.3899\n",
      "Epoch 29/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 344.1021 - mean_squared_error: 344.1021 - val_loss: 283.7493 - val_mean_squared_error: 283.7493\n",
      "Epoch 30/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 341.4948 - mean_squared_error: 341.4948 - val_loss: 236.2588 - val_mean_squared_error: 236.2588\n",
      "Epoch 31/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 357.1307 - mean_squared_error: 357.1307 - val_loss: 269.9156 - val_mean_squared_error: 269.9156\n",
      "Epoch 32/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 304.1395 - mean_squared_error: 304.1395 - val_loss: 330.8522 - val_mean_squared_error: 330.8522\n",
      "Epoch 33/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 353.2805 - mean_squared_error: 353.2805 - val_loss: 286.9367 - val_mean_squared_error: 286.9367\n",
      "Epoch 34/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 352.5754 - mean_squared_error: 352.5754 - val_loss: 244.4220 - val_mean_squared_error: 244.4220\n",
      "Epoch 35/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 368.0213 - mean_squared_error: 368.0213 - val_loss: 332.1714 - val_mean_squared_error: 332.1714\n",
      "Epoch 36/50\n",
      "101/101 [==============================] - 7s 68ms/step - loss: 329.3712 - mean_squared_error: 329.3712 - val_loss: 254.4374 - val_mean_squared_error: 254.4374\n",
      "Epoch 37/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 345.5510 - mean_squared_error: 345.5510 - val_loss: 249.4923 - val_mean_squared_error: 249.4923\n",
      "Epoch 38/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 335.9322 - mean_squared_error: 335.9322 - val_loss: 393.1358 - val_mean_squared_error: 393.1358\n",
      "Epoch 39/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 401.6785 - mean_squared_error: 401.6785 - val_loss: 282.3195 - val_mean_squared_error: 282.3195\n",
      "Epoch 40/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 363.9781 - mean_squared_error: 363.9781 - val_loss: 302.2940 - val_mean_squared_error: 302.2940\n",
      "Epoch 41/50\n",
      "101/101 [==============================] - 7s 70ms/step - loss: 339.0529 - mean_squared_error: 339.0529 - val_loss: 247.7754 - val_mean_squared_error: 247.7754\n",
      "Epoch 42/50\n",
      "101/101 [==============================] - 7s 70ms/step - loss: 373.3379 - mean_squared_error: 373.3379 - val_loss: 281.3137 - val_mean_squared_error: 281.3137\n",
      "Epoch 43/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 318.7036 - mean_squared_error: 318.7036 - val_loss: 342.1961 - val_mean_squared_error: 342.1961\n",
      "Epoch 44/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 329.0207 - mean_squared_error: 329.0207 - val_loss: 309.7986 - val_mean_squared_error: 309.7986\n",
      "Epoch 45/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 335.3218 - mean_squared_error: 335.3218 - val_loss: 215.8045 - val_mean_squared_error: 215.8045\n",
      "Epoch 46/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 314.4151 - mean_squared_error: 314.4151 - val_loss: 287.7177 - val_mean_squared_error: 287.7177\n",
      "Epoch 47/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 353.5271 - mean_squared_error: 353.5271 - val_loss: 319.3297 - val_mean_squared_error: 319.3297\n",
      "Epoch 48/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 329.5739 - mean_squared_error: 329.5739 - val_loss: 244.7385 - val_mean_squared_error: 244.7385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 303.6332 - mean_squared_error: 303.6332 - val_loss: 237.0814 - val_mean_squared_error: 237.0814\n",
      "Epoch 50/50\n",
      "101/101 [==============================] - 7s 69ms/step - loss: 325.6676 - mean_squared_error: 325.6676 - val_loss: 320.7943 - val_mean_squared_error: 320.7943\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13b9e2c1e20>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model.\n",
    "forecast_model.fit(t_gen, \n",
    "                   epochs = 50, \n",
    "                   verbose = 1, \n",
    "                   validation_data = v_gen, \n",
    "                   steps_per_epoch = len(meta_t) // batch_size,\n",
    "                   validation_steps = len(meta_v) // batch_size,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/005\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Models/005\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save trained model.\n",
    "forecast_model.save('Models/006')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine how model is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get data from one chip\n",
    "sample_input = np.load('Sample_Dataset_2/val/features/2144676.npy')\n",
    "\n",
    "# Move channel axis to the end.\n",
    "sample_input = np.moveaxis(sample_input, 0, -1)\n",
    "# Pad to even number of pixels\n",
    "a = np.pad(sample_input, [(0,0),(0,1),(0,1),(0,0)])\n",
    "# Resize to include a batch dimension.\n",
    "a = tf.expand_dims(a, axis = 0)\n",
    "\n",
    "# Get data from one chip\n",
    "sample_output = np.load('Sample_Dataset_2/val/labels/2144676.npy')\n",
    "\n",
    "# Pad to even number of pixels\n",
    "b = np.pad(sample_output, [(0,0),(0,1),(0,1)])\n",
    "# Resize to include a channel dimension.\n",
    "#b = tf.expand_dims(b, axis = -1)\n",
    "# Resize to include a batch dimension.\n",
    "#b = tf.expand_dims(b, axis = 0)\n",
    "b.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-8.827546"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_eval = forecast_model.predict(a)\n",
    "pred_eval[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "2.0\n",
      "-8.827546\n",
      "\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.0\n",
      "1.305102\n",
      "\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.0\n",
      "-8.8289585\n",
      "\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.0\n",
      "-8.836891\n",
      "\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.0\n",
      "-6.927685\n",
      "\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.0\n",
      "1.8310685\n",
      "\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "3\n",
      "-6.2787557\n",
      "\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "32\n",
      "-1.2721663\n",
      "\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.0\n",
      "-7.969244\n",
      "\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1.0\n",
      "7.294002\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_list = [\n",
    "    '2144676.npy',\n",
    "    '2200301.npy',\n",
    "    '2279930.npy',\n",
    "    '2323300.npy',\n",
    "    '2365430.npy',\n",
    "    '2461436.npy',\n",
    "    '2862603.npy',\n",
    "    '3159008.npy',\n",
    "    '3382322.npy',\n",
    "    '3439800.npy'\n",
    "]\n",
    "\n",
    "for x in range(0, 10):\n",
    "    a = np.load('Sample_Dataset_2/val/features/' + f_list[x])\n",
    "    a = np.moveaxis(a, 0, -1)\n",
    "    a = np.pad(a, [(0,0),(0,1),(0,1),(0,0)])\n",
    "    a = tf.expand_dims(a, axis = 0)\n",
    "    b = np.load('Sample_Dataset_2/val/labels/' + f_list[x])\n",
    "    pred_eval = forecast_model.predict(a)\n",
    "    print(b.sum())\n",
    "    print(pred_eval[0][0])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
