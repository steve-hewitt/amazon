{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import altair as alt\n",
    "import geopy.distance as gpx\n",
    "import geopandas as gpd\n",
    "from scipy.sparse import coo_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precip_indexer(t_lat, t_lon, t_time, t_date, t_year):\n",
    "    '''\n",
    "    Takes a desired location, date, time, and year; and returns the location of that row in the corresponding\n",
    "    precipation DataFrame. Assumes that data is stored in a certain predictably sorted order.\n",
    "    '''\n",
    "    base_dt = dt.datetime.strptime(str(int(t_year)) + '/01/01','%Y/%m/%d').date()\n",
    "    lat_idx = ((t_lat + 34) / .5)\n",
    "    lon_idx = ((t_lon + 73.75) / 0.625)\n",
    "    time_idx = (t_time)\n",
    "    date_idx = (t_date - base_dt).days\n",
    "    \n",
    "    return int((date_idx * 24 * 64 * 81) + (time_idx * 64 * 81) + (lon_idx * 81) + lat_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_with_defo_precip_stack(px_combined,\n",
    "                            defo_gpd,\n",
    "                            precip_data,\n",
    "                            centroid_lat = 0,\n",
    "                            centroid_lon = 0,\n",
    "                            centroid_date = dt.datetime.strptime('2017/01/01','%Y/%m/%d'),\n",
    "                            span = 15,\n",
    "                            days_back = 9,\n",
    "                            days_forward = 10):\n",
    "    \n",
    "    min_date = centroid_date - dt.timedelta(days = days_back)\n",
    "    max_date = centroid_date + dt.timedelta(days = days_forward)\n",
    "    t_year = centroid_date.year\n",
    "\n",
    "    # Calculate location if we move 375 meters North and use to get grid steps.\n",
    "    lat_375 = gpx.distance(kilometers=0.375).destination((centroid_lat, centroid_lon), bearing=0).latitude\n",
    "    lat_step = abs(centroid_lat - lat_375)\n",
    "\n",
    "    # Calculate location if we move 375 meters East and use to get grid steps.\n",
    "    lon_375 = gpx.distance(kilometers=0.375).destination((centroid_lat, centroid_lon), bearing=90).longitude\n",
    "    lon_step = abs(centroid_lon - lon_375)\n",
    "\n",
    "    # Get bounding box of centroid and span pixels in all directions.\n",
    "    min_lat = centroid_lat - (lat_step * (span + 1))\n",
    "    max_lat = centroid_lat + (lat_step * span)\n",
    "    min_lon = centroid_lon - (lon_step * (span + 1))\n",
    "    max_lon = centroid_lon + (lon_step * span)\n",
    "\n",
    "    # Filter data down to the range we want to plot\n",
    "    t_df = px_combined[(px_combined['Date'] >= min_date) & (px_combined['Date'] <= max_date)]\n",
    "    t_df = t_df[(t_df['Lat'] >= min_lat) & (t_df['Lat'] <= max_lat) & (t_df['Lon'] >= min_lon) & (t_df['Lon'] <= max_lon)]\n",
    "\n",
    "    # Calculate pixel locations in the new grid.\n",
    "    t_df['delta_lat'] = t_df['Lat'] - centroid_lat\n",
    "    t_df['delta_lon'] = t_df['Lon'] - centroid_lon\n",
    "    t_df['lat_grid'] = t_df['delta_lat'] // lat_step\n",
    "    t_df['lon_grid'] = t_df['delta_lon'] // lon_step\n",
    "\n",
    "    # Express date difference\n",
    "    t_df['delta_day'] = (t_df['Date'] - centroid_date).dt.days\n",
    "\n",
    "    # Convert grids to not have negative locations.\n",
    "    t_df['lat_grid'] = t_df['lat_grid'] + span + 1\n",
    "    t_df['lon_grid'] = t_df['lon_grid'] + span + 1\n",
    "\n",
    "    ### Begin added code for deforestation processing.\n",
    "\n",
    "    # Limit deforestation polygons to those that intersect with centroid grid.\n",
    "    centroid_defo = defo_gpd[(defo_gpd['x_min'] >= min_lon) & (defo_gpd['x_max'] <= max_lon) &\n",
    "                   (defo_gpd['y_min'] >= min_lat) & (defo_gpd['y_max'] <= max_lat) &\n",
    "                   (defo_gpd['x_min'] + defo_gpd['x_min'] + defo_gpd['x_min'] + defo_gpd['x_min'] != 0) &\n",
    "                   (defo_gpd['Date'] <= min_date)]\n",
    "\n",
    "    if len(centroid_defo) > 0:\n",
    "        # Build grid.\n",
    "        lats = [min_lat + lat_step * x for x in range(0,span * 2 + 2)]\n",
    "        lons = [min_lon + lon_step * x for x in range(0,span * 2 + 2)]\n",
    "        cs = [c for c in range(0,span * 2 + 2)]\n",
    "        grid = list(itertools.product(lons, lats))\n",
    "        grid_xy = list(itertools.product(cs, cs))\n",
    "        grid_points = gpd.points_from_xy([x[0] for x in grid], [y[1] for y in grid])\n",
    "        grid_gdf = gpd.GeoDataFrame(grid_points)\n",
    "        grid_gdf.rename(mapper={0:'geometry'}, axis=1, inplace = True)\n",
    "        grid_gdf['x'] = [c[0] for c in grid_xy]\n",
    "        grid_gdf['y'] = [c[1] for c in grid_xy]\n",
    "        grid_gdf = grid_gdf.set_geometry('geometry')\n",
    "        grid_gdf.crs = 'epsg:4674'\n",
    "\n",
    "        # Calculate deforested gridpoints.\n",
    "        grid_gdf['deforested'] = np.minimum(pd.concat(\n",
    "            [grid_gdf.within(centroid_defo.iloc[x].geometry.buffer(0)) for x in range(0,len(centroid_defo))]\n",
    "            , axis=1).sum(1),1)\n",
    "\n",
    "        # Convert to numpy format.\n",
    "        dm = coo_matrix((grid_gdf['deforested'],(grid_gdf['x'].astype(int),grid_gdf['y'].astype(int)))).toarray()\n",
    "    else:\n",
    "        dm = np.zeros(((2 * span + 2),(2 * span + 2)))\n",
    "\n",
    "    ### End added code for deforestaion processing.\n",
    "    \n",
    "    ### Begin added code for precip processing.\n",
    "    w_lat_min = centroid_lat // 0.5 * 0.5\n",
    "    w_lat_max = w_lat_min + 0.5\n",
    "    w_lon_min = centroid_lon // 0.625 * 0.625\n",
    "    w_lon_max = w_lon_min + 0.625\n",
    "    wt = []\n",
    "    \n",
    "    for x in range(-days_back, 1):\n",
    "        t_date = (centroid_date + dt.timedelta(days = x)).date()\n",
    "        \n",
    "        w1 = np.concatenate([precip_data.iloc[precip_indexer(w_lat_min, w_lon_min, x, t_date, t_year)].values.flatten() for x in range(0,24)])\n",
    "        w2 = np.concatenate([precip_data.iloc[precip_indexer(w_lat_max, w_lon_min, x, t_date, t_year)].values.flatten() for x in range(0,24)])\n",
    "        w3 = np.concatenate([precip_data.iloc[precip_indexer(w_lat_min, w_lon_max, x, t_date, t_year)].values.flatten() for x in range(0,24)])\n",
    "        w4 = np.concatenate([precip_data.iloc[precip_indexer(w_lat_max, w_lon_max, x, t_date, t_year)].values.flatten() for x in range(0,24)])\n",
    "\n",
    "        wt.append(np.mean([w1,w2,w3,w4], axis = 0))\n",
    "        \n",
    "    wt = np.stack(wt)\n",
    "    ### End added code for precip processing.\n",
    "\n",
    "    # Build an array for each day in our date range then stack them. One for the inputs and one for the outputs.\n",
    "    input_stack = []\n",
    "    for x in range(-days_back, 1):\n",
    "        s_df = t_df[t_df['Date'] == (centroid_date + dt.timedelta(days = x))]\n",
    "        if len(s_df) > 0:\n",
    "            cm = coo_matrix((s_df['value'],(s_df['lon_grid'].astype(int),s_df['lat_grid'].astype(int)))).toarray()\n",
    "            cm = np.pad(cm, pad_width=((0, (2 * span + 2) - cm.shape[0]),(0,(2 * span + 2) - cm.shape[1])), mode = 'constant')\n",
    "        else:\n",
    "            cm = np.zeros(((2 * span + 2),(2 * span + 2)))\n",
    "        input_stack.append(cm)\n",
    "    # Stack fires/defo.\n",
    "    input_stack = np.stack([np.stack(input_stack), np.stack([dm]*10)], axis = -1)\n",
    "    \n",
    "    output_stack = []\n",
    "    for x in range(1, days_forward + 1):\n",
    "        s_df = t_df[t_df['Date'] == (centroid_date + dt.timedelta(days = x))]\n",
    "        if len(s_df) > 0:\n",
    "            cm = coo_matrix((s_df['value'],(s_df['lon_grid'].astype(int),s_df['lat_grid'].astype(int)))).toarray()\n",
    "            cm = np.pad(cm, pad_width=((0, (2 * span + 2) - cm.shape[0]),(0,(2 * span + 2) - cm.shape[1])), mode = 'constant')\n",
    "        else:\n",
    "            cm = np.zeros(((2 * span + 2),(2 * span + 2)))\n",
    "        output_stack.append(np.stack(cm))\n",
    "    output_stack = np.stack(output_stack)\n",
    "        \n",
    "        \n",
    "    return input_stack, wt, output_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_with_defo_precip_stack_and_save(\n",
    "                 centroid_df,\n",
    "                 defo_gpd,\n",
    "                 precip_data,\n",
    "                 px_combined,\n",
    "                 meta_folder = '/data/',\n",
    "                 feature_folder_2d = '/data/features_2d/',\n",
    "                 feature_folder_1d = '/data/features_1d/',\n",
    "                 label_folder = '/data/labels/',\n",
    "                 span = 15,\n",
    "                 days_back = 10,\n",
    "                 days_forward = 10):\n",
    "    \n",
    "    meta_df = centroid_df.copy()\n",
    "    meta_df['features_2d'] = feature_folder_2d + meta_df.index.astype(str) + '.npy'\n",
    "    meta_df['features_1d'] = feature_folder_1d + meta_df.index.astype(str) + '.npy'\n",
    "    meta_df['labels'] = label_folder + meta_df.index.astype(str) + '.npy'\n",
    "    \n",
    "    for x in range(0,len(centroid_df)):\n",
    "        \n",
    "        t_index = centroid_df.index[x].astype(str)\n",
    "        centroid_lat = centroid_df.iloc[x].Lat\n",
    "        centroid_lon = centroid_df.iloc[x].Lon\n",
    "        centroid_date = centroid_df.iloc[x].Date\n",
    "        t_year = centroid_date.year\n",
    "        \n",
    "        min_date = centroid_date - dt.timedelta(days = days_back)\n",
    "        max_date = centroid_date + dt.timedelta(days = days_forward)\n",
    "\n",
    "        # Calculate location if we move 375 meters North and use to get grid steps.\n",
    "        lat_375 = gpx.distance(kilometers=0.375).destination((centroid_lat, centroid_lon), bearing=0).latitude\n",
    "        lat_step = abs(centroid_lat - lat_375)\n",
    "\n",
    "        # Calculate location if we move 375 meters East and use to get grid steps.\n",
    "        lon_375 = gpx.distance(kilometers=0.375).destination((centroid_lat, centroid_lon), bearing=90).longitude\n",
    "        lon_step = abs(centroid_lon - lon_375)\n",
    "\n",
    "        # Get bounding box of centroid and span pixels in all directions.\n",
    "        min_lat = centroid_lat - (lat_step * (span + 1))\n",
    "        max_lat = centroid_lat + (lat_step * span)\n",
    "        min_lon = centroid_lon - (lon_step * (span + 1))\n",
    "        max_lon = centroid_lon + (lon_step * span)\n",
    "        \n",
    "        # Filter data down to the range we want to plot\n",
    "        t_df = px_combined[(px_combined['Date'] >= min_date) & (px_combined['Date'] <= max_date)]\n",
    "        t_df = t_df[(t_df['Lat'] >= min_lat) & (t_df['Lat'] <= max_lat) & (t_df['Lon'] >= min_lon) & (t_df['Lon'] <= max_lon)]\n",
    "\n",
    "        # Calculate pixel locations in the new grid.\n",
    "        t_df['delta_lat'] = t_df['Lat'] - centroid_lat\n",
    "        t_df['delta_lon'] = t_df['Lon'] - centroid_lon\n",
    "        t_df['lat_grid'] = t_df['delta_lat'] // lat_step\n",
    "        t_df['lon_grid'] = t_df['delta_lon'] // lon_step\n",
    "\n",
    "        # Express date difference\n",
    "        t_df['delta_day'] = (t_df['Date'] - centroid_date).dt.days\n",
    "\n",
    "        # Convert grids to not have negative locations.\n",
    "        t_df['lat_grid'] = t_df['lat_grid'] + span + 1\n",
    "        t_df['lon_grid'] = t_df['lon_grid'] + span + 1\n",
    "        \n",
    "        ### Begin added code for deforestation processing.\n",
    "        \n",
    "        # Limit deforestation polygons to those that intersect with centroid grid.\n",
    "        centroid_defo = defo_gpd[(defo_gpd['x_min'] >= min_lon) & (defo_gpd['x_max'] <= max_lon) &\n",
    "                       (defo_gpd['y_min'] >= min_lat) & (defo_gpd['y_max'] <= max_lat) &\n",
    "                       (defo_gpd['x_min'] + defo_gpd['x_min'] + defo_gpd['x_min'] + defo_gpd['x_min'] != 0) &\n",
    "                       (defo_gpd['Date'] <= min_date)]\n",
    "        \n",
    "        if len(centroid_defo) > 0:\n",
    "            # Build grid.\n",
    "            lats = [min_lat + lat_step * x for x in range(0,span * 2 + 2)]\n",
    "            lons = [min_lon + lon_step * x for x in range(0,span * 2 + 2)]\n",
    "            cs = [c for c in range(0,span * 2 + 2)]\n",
    "            grid = list(itertools.product(lons, lats))\n",
    "            grid_xy = list(itertools.product(cs, cs))\n",
    "            grid_points = gpd.points_from_xy([x[0] for x in grid], [y[1] for y in grid])\n",
    "            grid_gdf = gpd.GeoDataFrame(grid_points)\n",
    "            grid_gdf.rename(mapper={0:'geometry'}, axis=1, inplace = True)\n",
    "            grid_gdf['x'] = [c[0] for c in grid_xy]\n",
    "            grid_gdf['y'] = [c[1] for c in grid_xy]\n",
    "            grid_gdf = grid_gdf.set_geometry('geometry')\n",
    "            grid_gdf.crs = 'epsg:4674'\n",
    "\n",
    "            # Calculate deforested gridpoints.\n",
    "            grid_gdf['deforested'] = np.minimum(pd.concat(\n",
    "                [grid_gdf.within(centroid_defo.iloc[x].geometry.buffer(0)) for x in range(0,len(centroid_defo))]\n",
    "                , axis=1).sum(1),1)\n",
    "\n",
    "            # Convert to numpy format.\n",
    "            dm = coo_matrix((grid_gdf['deforested'],(grid_gdf['x'].astype(int),grid_gdf['y'].astype(int)))).toarray()\n",
    "        else:\n",
    "            dm = np.zeros(((2 * span + 2),(2 * span + 2)))\n",
    "        \n",
    "        ### End added code for deforestaion processing.\n",
    "        \n",
    "        ### Begin added code for precip processing.\n",
    "        w_lat_min = centroid_lat // 0.5 * 0.5\n",
    "        w_lat_max = w_lat_min + 0.5\n",
    "        w_lon_min = centroid_lon // 0.625 * 0.625\n",
    "        w_lon_max = w_lon_min + 0.625\n",
    "        wt = []\n",
    "        \n",
    "        for x in range(-days_back, 1):\n",
    "            t_date = (centroid_date + dt.timedelta(days = x)).date()\n",
    "\n",
    "            w1 = np.concatenate([precip_data.iloc[precip_indexer(w_lat_min, w_lon_min, x, t_date, t_year)].values.flatten() for x in range(0,24)])\n",
    "            w2 = np.concatenate([precip_data.iloc[precip_indexer(w_lat_max, w_lon_min, x, t_date, t_year)].values.flatten() for x in range(0,24)])\n",
    "            w3 = np.concatenate([precip_data.iloc[precip_indexer(w_lat_min, w_lon_max, x, t_date, t_year)].values.flatten() for x in range(0,24)])\n",
    "            w4 = np.concatenate([precip_data.iloc[precip_indexer(w_lat_max, w_lon_max, x, t_date, t_year)].values.flatten() for x in range(0,24)])\n",
    "\n",
    "            wt.append(np.mean([w1,w2,w3,w4], axis = 0))\n",
    "\n",
    "        wt = np.stack(wt)\n",
    "        ### End added code for precip processing.\n",
    "\n",
    "        # Build an array for each day in our date range then stack them. One for the inputs and one for the outputs.\n",
    "        input_stack = []\n",
    "        for x in range(-days_back, 1):\n",
    "            s_df = t_df[t_df['Date'] == (centroid_date + dt.timedelta(days = x))]\n",
    "            if len(s_df) > 0:\n",
    "                cm = coo_matrix((s_df['value'],(s_df['lon_grid'].astype(int),s_df['lat_grid'].astype(int)))).toarray()\n",
    "                cm = np.pad(cm, pad_width=((0, (2 * span + 2) - cm.shape[0]),(0,(2 * span + 2) - cm.shape[1])), mode = 'constant')\n",
    "            else:\n",
    "                cm = np.zeros(((2 * span + 2),(2 * span + 2)))\n",
    "            input_stack.append(cm)\n",
    "        # New stack logic where we layer fires/defo/wind-U/wind-V and fixed channel ordering.\n",
    "        input_stack = np.stack([np.stack(input_stack), np.stack([dm]*10)], axis = -1)\n",
    "\n",
    "        output_stack = []\n",
    "        for x in range(1, days_forward + 1):\n",
    "            s_df = t_df[t_df['Date'] == (centroid_date + dt.timedelta(days = x))]\n",
    "            if len(s_df) > 0:\n",
    "                cm = coo_matrix((s_df['value'],(s_df['lon_grid'].astype(int),s_df['lat_grid'].astype(int)))).toarray()\n",
    "                cm = np.pad(cm, pad_width=((0, (2 * span + 2) - cm.shape[0]),(0,(2 * span + 2) - cm.shape[1])), mode = 'constant')\n",
    "            else:\n",
    "                cm = np.zeros(((2 * span + 2),(2 * span + 2)))\n",
    "            output_stack.append(np.stack(cm))\n",
    "        output_stack = np.stack(output_stack)\n",
    "        \n",
    "        ## Save to disk\n",
    "        np.save(feature_folder_2d + t_index + '.npy', input_stack)\n",
    "        np.save(feature_folder_1d + t_index + '.npy', wt)\n",
    "        np.save(label_folder + t_index + '.npy', output_stack)\n",
    "        \n",
    "    meta_df.to_csv(meta_folder + 'meta.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get deforestation data from shapefile.\n",
    "defo_gpd = gpd.read_file('Yearly_Deforestation_Biome/yearly_deforestation_biome.shp')\n",
    "\n",
    "# Filter to deforestation shapes in Para.\n",
    "defo_gpd = defo_gpd[defo_gpd['state'] == 'PA']\n",
    "\n",
    "# Add bounding boxes to DataFrame.\n",
    "gpd_bbox = defo_gpd.bounds\n",
    "defo_gpd['x_min'] = gpd_bbox['minx']\n",
    "defo_gpd['x_max'] = gpd_bbox['maxx']\n",
    "defo_gpd['y_min'] = gpd_bbox['miny']\n",
    "defo_gpd['y_max'] = gpd_bbox['maxy']\n",
    "\n",
    "# Convert date format.\n",
    "defo_gpd['Date'] = pd.to_datetime(defo_gpd['image_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VIIRS fire data.\n",
    "px_combined = pd.read_csv('VIIRS_Para.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust date format.\n",
    "px_combined['Date'] = pd.to_datetime(px_combined['YYYYMMDD'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add month column so we can sample from September only.\n",
    "px_combined['Month'] = px_combined['Date'].dt.month\n",
    "\n",
    "# Add year column so we can segregate train/val/test by year.\n",
    "px_combined['Year'] = px_combined['Date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column where every value is a 1 to use in 1-hot encoding strategy.\n",
    "px_combined['value'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFeature translation for precipation data:\\nPRECTOTCORR - bias corrected total precipitation\\nQLML - surface specific humidity\\nQSH - effective surface specific humidity\\nQSTAR - surface moisutre scale\\nRHOA - air density at surface\\nSPEED - surface wind speed\\nTLML - surface air temperature\\nULML - surface eastward wind\\nVLML - surface northward wind\\n\\nIndices: date, time, lat, lon\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load new precipiation data that also includes wind.\n",
    "# Samples exist every 0.5 degrees lat and 0.625 degrees lon.\n",
    "# Warning: this cell requires ~64 GB of RAM to run.\n",
    "'''\n",
    "Feature translation for precipation data:\n",
    "PRECTOTCORR - bias corrected total precipitation\n",
    "QLML - surface specific humidity\n",
    "QSH - effective surface specific humidity\n",
    "QSTAR - surface moisutre scale\n",
    "RHOA - air density at surface\n",
    "SPEED - surface wind speed\n",
    "TLML - surface air temperature\n",
    "ULML - surface eastward wind\n",
    "VLML - surface northward wind\n",
    "\n",
    "Indices: date, time, lat, lon\n",
    "'''\n",
    "#precip_data = pd.concat([pd.read_csv('precipitation-data/combined_precip_data_' + str(x) + '.csv') for x in range(2017,2022)])\n",
    "#precip_data.drop(columns = ['Unnamed: 0','SPEED'], inplace=True)\n",
    "#precip_data['date'] = pd.to_datetime(precip_data['date'], format='%Y%m%d').dt.date\n",
    "#precip_data = precip_data.set_index(['date','lat','lon','Time_Hour'])\n",
    "\n",
    "### Need to break this up by year to improve runtime. Code moved to the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets (4-Fold Split with precipitation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded... proceeding with processing\n",
      "Wall time: 19min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build 2017 set.\n",
    "df_a = px_combined[(px_combined['Month'] > 7) & (px_combined['Month'] < 11) & (px_combined['Year'] == 2017)].sample(n=5000, random_state=321)[['Lon','Lat','Date']]\n",
    "\n",
    "# Load relevant precipitation data.\n",
    "precip_data = pd.read_csv('precipitation-data/combined_precip_data_2017.csv')\n",
    "precip_data.drop(columns = ['Unnamed: 0','SPEED'], inplace=True)\n",
    "precip_data['date'] = pd.to_datetime(precip_data['date'], format='%Y%m%d').dt.date\n",
    "precip_data = precip_data.set_index(['date','lat','lon','Time_Hour'])\n",
    "\n",
    "one_hot_with_defo_precip_stack_and_save(df_a, \n",
    "                                 defo_gpd,\n",
    "                                 precip_data[['PRECTOTCORR', 'QLML', 'QSH', 'QSTAR', 'RHOA', 'TLML','ULML', 'VLML']],\n",
    "                                 px_combined, \n",
    "                                 '4fold_super/2017/', \n",
    "                                 '4fold_super/2017/features_2d/', \n",
    "                                 '4fold_super/2017/features_1d/', \n",
    "                                 '4fold_super/2017/labels/',\n",
    "                                 span = 15,\n",
    "                                 days_back = 9,\n",
    "                                 days_forward = 1\n",
    "                                 )\n",
    "\n",
    "# Erase precipiation data to save memory.\n",
    "precip_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build 2018 set.\n",
    "df_b = px_combined[(px_combined['Month'] > 7) & (px_combined['Month'] < 11) & (px_combined['Year'] == 2018)].sample(n=5000, random_state=321)[['Lon','Lat','Date']]\n",
    "\n",
    "# Load relevant precipitation data.\n",
    "precip_data = pd.read_csv('precipitation-data/combined_precip_data_2018.csv')\n",
    "precip_data.drop(columns = ['Unnamed: 0','SPEED'], inplace=True)\n",
    "precip_data['date'] = pd.to_datetime(precip_data['date'], format='%Y%m%d').dt.date\n",
    "precip_data = precip_data.set_index(['date','lat','lon','Time_Hour'])\n",
    "\n",
    "one_hot_with_defo_precip_stack_and_save(df_b, \n",
    "                                 defo_gpd,\n",
    "                                 precip_data[['PRECTOTCORR', 'QLML', 'QSH', 'QSTAR', 'RHOA', 'TLML','ULML', 'VLML']],\n",
    "                                 px_combined,\n",
    "                                 '4fold_super/2018/', \n",
    "                                 '4fold_super/2018/features_2d/', \n",
    "                                 '4fold_super/2018/features_1d/',  \n",
    "                                 '4fold_super/2018/labels/',\n",
    "                                 span = 15,\n",
    "                                 days_back = 9,\n",
    "                                 days_forward = 1\n",
    "                                 )\n",
    "\n",
    "# Erase precipiation data to save memory.\n",
    "precip_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build 2019 set.\n",
    "df_c = px_combined[(px_combined['Month'] > 7) & (px_combined['Month'] < 11) & (px_combined['Year'] == 2019)].sample(n=5000, random_state=321)[['Lon','Lat','Date']]\n",
    "\n",
    "# Load relevant precipitation data.\n",
    "precip_data = pd.read_csv('precipitation-data/combined_precip_data_2019.csv')\n",
    "precip_data.drop(columns = ['Unnamed: 0','SPEED'], inplace=True)\n",
    "precip_data['date'] = pd.to_datetime(precip_data['date'], format='%Y%m%d').dt.date\n",
    "precip_data = precip_data.set_index(['date','lat','lon','Time_Hour'])\n",
    "\n",
    "one_hot_with_defo_precip_stack_and_save(df_c, \n",
    "                                 defo_gpd,\n",
    "                                 precip_data[['PRECTOTCORR', 'QLML', 'QSH', 'QSTAR', 'RHOA', 'TLML','ULML', 'VLML']],\n",
    "                                 px_combined, \n",
    "                                 '4fold_super/2019/', \n",
    "                                 '4fold_super/2019/features_2d/', \n",
    "                                 '4fold_super/2019/features_1d/',  \n",
    "                                 '4fold_super/2019/labels/',\n",
    "                                 span = 15,\n",
    "                                 days_back = 9,\n",
    "                                 days_forward = 1\n",
    "                                 )\n",
    "\n",
    "# Erase precipiation data to save memory.\n",
    "precip_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build 2020 set.\n",
    "df_d = px_combined[(px_combined['Month'] > 7) & (px_combined['Month'] < 11) & (px_combined['Year'] == 2020)].sample(n=5000, random_state=321)[['Lon','Lat','Date']]\n",
    "\n",
    "# Load relevant precipitation data.\n",
    "precip_data = pd.read_csv('precipitation-data/combined_precip_data_2020.csv')\n",
    "precip_data.drop(columns = ['Unnamed: 0','SPEED'], inplace=True)\n",
    "precip_data['date'] = pd.to_datetime(precip_data['date'], format='%Y%m%d').dt.date\n",
    "precip_data = precip_data.set_index(['date','lat','lon','Time_Hour'])\n",
    "\n",
    "one_hot_with_defo_precip_stack_and_save(df_d, \n",
    "                                 defo_gpd,\n",
    "                                 precip_data[['PRECTOTCORR', 'QLML', 'QSH', 'QSTAR', 'RHOA', 'TLML','ULML', 'VLML']],\n",
    "                                 px_combined, \n",
    "                                 '4fold_super/2020/', \n",
    "                                 '4fold_super/2020/features_2d/', \n",
    "                                 '4fold_super/2020/features_1d/',  \n",
    "                                 '4fold_super/2020/labels/',\n",
    "                                 span = 15,\n",
    "                                 days_back = 9,\n",
    "                                 days_forward = 1\n",
    "                                 )\n",
    "\n",
    "# Erase precipiation data to save memory.\n",
    "precip_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Build 2021 set.\n",
    "df_e = px_combined[(px_combined['Month'] > 7) & (px_combined['Month'] < 11) & (px_combined['Year'] == 2021)].sample(n=5000, random_state=321)[['Lon','Lat','Date']]\n",
    "\n",
    "# Load relevant precipitation data.\n",
    "precip_data = pd.read_csv('precipitation-data/combined_precip_data_2021.csv')\n",
    "precip_data.drop(columns = ['Unnamed: 0','SPEED'], inplace=True)\n",
    "precip_data['date'] = pd.to_datetime(precip_data['date'], format='%Y%m%d').dt.date\n",
    "precip_data = precip_data.set_index(['date','lat','lon','Time_Hour'])\n",
    "\n",
    "one_hot_with_defo_precip_stack_and_save(df_e, \n",
    "                                 defo_gpd,\n",
    "                                 precip_data[['PRECTOTCORR', 'QLML', 'QSH', 'QSTAR', 'RHOA', 'TLML','ULML', 'VLML']],\n",
    "                                 px_combined, \n",
    "                                 '4fold_super/2021/', \n",
    "                                 '4fold_super/2021/features_2d/', \n",
    "                                 '4fold_super/2021/features_1d/',  \n",
    "                                 '4fold_super/2021/labels/',\n",
    "                                 span = 15,\n",
    "                                 days_back = 9,\n",
    "                                 days_forward = 1\n",
    "                                 )\n",
    "\n",
    "# Erase precipiation data to save memory.\n",
    "precip_data = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
